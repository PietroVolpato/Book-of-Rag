# src/galoispy/utils/chat_utils.py
import json
from pydantic import ValidationError
import re
import tiktoken
from typing import Tuple
# LangChain packages
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import Runnable
from langchain_openai.chat_models.base import OpenAIRefusalError
# LLama-Index packages
from llama_index.core.memory import ChatMemoryBuffer
# Utils
from galoispy.exception import NotStructuredAnswerError

def _strip_think_section(text: str) -> str:
    """
    Helper function to remove the <think> section from a model response.
    
    :param text: The text from which to remove the <think> section.
    :type text: str
    :return: The text with the <think> section removed.
    :rtype: str
    
    """
    try:
        json_content = json.loads(text)
        text = json_content.get("content", "")
    except json.JSONDecodeError:
        pass
    return re.sub(r"<think>.*?</think>\s*", "", text, flags=re.DOTALL | re.IGNORECASE)

def memory_init(system_prompt: str) -> ChatMemoryBuffer:
    """
    It initializes the chat memory with a system prompt.
    
    :param system_prompt: The system prompt to initialize the memory.
    :type system_prompt: str
    :return: The initialized chat memory.
    :rtype: ChatMemoryBuffer
    """
    memory = ChatMemoryBuffer.from_defaults()
    if len(system_prompt) > 0:
        memory.put(SystemMessage(content=system_prompt))
    return memory

def _memory_near_limit(memory: ChatMemoryBuffer, context_window_size: int, occupancy_ratio: float, tokenizer: str) -> bool:
    """
    It checks if the memory is near to the maximum token limit.
    
    :param memory: The chat memory buffer.
    :type memory: ChatMemoryBuffer
    :param context_window_size: The context window size of the model.
    :type context_window_size: int
    :param occupancy_ratio: The occupancy ratio to determine the threshold.
    :type occupancy_ratio: float
    :param tokenizer: The tokenizer to use for counting tokens.
    :type tokenizer: str
    :return: True if the memory is near the token limit, False otherwise.
    :rtype: bool
    """
    history_messages = memory.get_all()
    conversation = ""
    for msg in history_messages:
        # Get messages from Llama-index (they are saved as dictionary)
        if hasattr(msg, "role") and hasattr(msg, "content"):
            if msg.role == "user":
                conversation = conversation + str(msg.content)
            elif msg.role == "assistant":
                conversation = conversation + str(msg.content)
            else:
                conversation = conversation + str(msg.content)
        if isinstance(msg, SystemMessage):
            system_prompt = msg.content
        elif isinstance(msg, HumanMessage) or isinstance(msg, AIMessage):
            conversation = conversation + str(msg.content)

        # Count tokens
        encoding = None
        try:
            encoding = tiktoken.get_encoding(tokenizer)
        except ValueError:
            encoding = tiktoken.get_encoding("cl100k_base")
        # Calculate the number of tokens
        num_tokens = len(encoding.encode(conversation))
        return num_tokens >= context_window_size * occupancy_ratio
    
def _get_num_tokens(messages: list, tokenizer: str) -> int:
    """
    Calculate the number of tokens in a list of messages using the specified tokenizer.
    
    :param messages: The list of messages to count tokens for.
    :type messages: list
    :param tokenizer: The tokenizer to use for counting tokens.
    :type tokenizer: str
    :return: The total number of tokens in the messages.
    :rtype: int
    """
    num_token = 0
    for message in messages:
        encoding = None
        try:
            encoding = tiktoken.get_encoding(tokenizer)
        except ValueError:
            encoding = tiktoken.get_encoding("cl100k_base")
        num_token += len(encoding.encode(message.content))

    return num_token

def _memory_refactor(memory: ChatMemoryBuffer, summarization_prompt_template: str) -> ChatMemoryBuffer:
    """
    It summarizes the chat history to reduce token usage.\n
    This function is invoked when the memory is near to the maximum token limit.
    
    :param memory: The chat memory buffer.
    :type memory: ChatMemoryBuffer
    :param summarization_prompt_template: The prompt template for summarization.
    :type summarization_prompt_template: str
    :return: The refactored chat memory buffer.
    :rtype: ChatMemoryBuffer
    """
    # Get all messages from memory
    history_messages = memory.get_all()
    generated_content = ""
    system_prompt = ""
    for msg in history_messages:
        # Get only the messages generated by the LLM
        # The LLM generates only JSON responses
        if hasattr(msg, "role") and hasattr(msg, "content"):
            if msg.role == "system" and not system_prompt:
                system_prompt = msg.content
            elif msg.role == "assistant":
                generated_content += "\n" + str(msg.content)
        elif isinstance(msg, SystemMessage) and not system_prompt:
            system_prompt = msg.content
        elif isinstance(msg, AIMessage):
            generated_content += "\n" + str(msg.content)

    # Create a prompt to summarize the generated content
    memory = ChatMemoryBuffer.from_defaults()
    memory.put(SystemMessage(content=summarization_prompt_template.format(system_prompt=system_prompt, generated_content=generated_content.strip())))
    return memory

def get_structured_response(model: Runnable, user_prompt: str, system_prompt: str, tokenizer: str) -> Tuple[str, int]:
    """
    Helper function to get structured response **without** a chat memory buffer.
    
    :param model: The LLM model to use.
    :type model: Runnable
    :param user_prompt: The user prompt to send to the model.
    :type user_prompt: str
    :param system_prompt: The system prompt to provide context to the model.
    :type system_prompt: str
    :param tokenizer: The tokenizer to use for counting tokens.
    :type tokenizer: str
    :return: A tuple containing the structured response and the number of tokens used.
    :rtype: Tuple[str, int]
    """
    tokens_used = 0
    messages = []
    
    # Add system prompt if present
    if system_prompt:
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
    else:
        messages = [
            HumanMessage(content=user_prompt)
        ]

    response = None
    for i in range(3): # Retry up to 3 times
        try:
            response = model.invoke(messages)
            break
        # Catch langchain exception - LLM output is not structured
        except (AttributeError, ValidationError, OpenAIRefusalError):
            # Update token usage
            tokens_used = _get_num_tokens(messages=messages, tokenizer=tokenizer)
            continue

    if response is None:
        raise NotStructuredAnswerError("The model did not return a valid response after multiple attempts. Please try again later.")
    
    # Calculate token usage with the response
    if hasattr(response, "model_dump_json"):
        content_str = response.model_dump_json()
    elif hasattr(response, "content"):
        content_str = str(response.content)
    else:
        content_str = str(response)
    messages.append(AIMessage(content=content_str))
    tokens_used = _get_num_tokens(messages=messages, tokenizer=tokenizer)

    return (response, tokens_used)

def get_structured_response_with_history(model: Runnable, memory: ChatMemoryBuffer, user_prompt: str, summarization_prompt_template: str, context_window_size: int, occupancy_ratio: float, tokenizer: str) -> Tuple[str, ChatMemoryBuffer, int]:
    """
    Helper function to get structured response **with** a chat memory buffer.
    
    :param model: The LLM model to use.
    :type model: Runnable
    :param memory: The chat memory buffer to use.
    :type memory: ChatMemoryBuffer
    :param user_prompt: The user prompt to send to the model.
    :type user_prompt: str
    :param summarization_prompt_template: The template to use for summarizing the memory.
    :type summarization_prompt_template: str
    :param context_window_size: The size of the context window.
    :type context_window_size: int
    :param occupancy_ratio: The occupancy ratio to determine when to summarize memory.
    :type occupancy_ratio: float
    :param tokenizer: The tokenizer to use for counting tokens.
    :type tokenizer: str
    :return: A tuple containing the structured response, the updated memory buffer, and the number of tokens used.
    :rtype: Tuple[str, ChatMemoryBuffer, int]
    """
    tokens_used = 0
    # Check if memory is near to the limit
    if _memory_near_limit(memory=memory, context_window_size=context_window_size, occupancy_ratio=occupancy_ratio, tokenizer=tokenizer):
        memory = _memory_refactor(memory=memory, summarization_prompt_template=summarization_prompt_template)

    history_messages = memory.get_all()
    formatted_history = []
    system_prompt = ""
    for msg in history_messages:
        # Get messages from Llama-index (they are saved as dictionary)
        if hasattr(msg, "role") and hasattr(msg, "content"):
            if msg.role == "user":
                formatted_history.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                formatted_history.append(AIMessage(content=msg.content))
            elif msg.role == "system":
                system_prompt = msg.content
        elif isinstance(msg, SystemMessage):
            system_prompt = msg.content
        elif isinstance(msg, HumanMessage) or isinstance(msg, AIMessage):
            formatted_history.append(msg)
    
    messages = []

    # Add system prompt if present
    if len(system_prompt) > 0: 
        messages = [
            SystemMessage(content=system_prompt),
            *formatted_history,
            HumanMessage(content=user_prompt)
        ]
    else:
        messages = [
            *formatted_history,
            HumanMessage(content=user_prompt)
        ]

    response = None
    for i in range(3): # Retry up to 3 times
        try:
            response = model.invoke(messages)
            break
        # Catch langchain exception - LLM output is not structured
        except (AttributeError, ValidationError, OpenAIRefusalError):
            # Update token usage
            tokens_used = _get_num_tokens(messages=messages, tokenizer=tokenizer)
            continue
    
    if response is None:
        raise NotStructuredAnswerError("The model did not return a valid response after multiple attempts. Please try again later.")
    
    # Update token usage with the response
    if hasattr(response, "model_dump_json"):
        content_str = response.model_dump_json()
    elif hasattr(response, "content"):
        content_str = str(response.content)
    else:
        content_str = str(response)
    messages.append(AIMessage(content=content_str))
    tokens_used += _get_num_tokens(messages=messages, tokenizer=tokenizer)
    
    # Remove the <think> section from the response (if present)
    # It is used for decreasing the token usage in the memory
    clean_response = ""
    if hasattr(response, "model_dump_json"):
        clean_response = _strip_think_section(text=response.model_dump_json())
    elif hasattr(response, "content"):
        clean_response = _strip_think_section(text=response.content)
    else:
        clean_response = _strip_think_section(str(response))

    # Save the last interaction in memory
    memory.put(HumanMessage(content=user_prompt))
    memory.put(AIMessage(content=clean_response if clean_response else response if isinstance(response, str) else (response.content if hasattr(response, "content") else str(response))))

    return (response, memory, tokens_used)